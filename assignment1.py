# -*- coding: utf-8 -*-
"""assignment1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bLMqWHRlnwtP50m1V8tmio1puakXbmO2
"""

#Anuj Tadkase
#date=17-07-2023
#roll number -64
#prn:12211778



import numpy
# Import pandas package
import pandas as pd

# Assign data
data = {'Books': ['tintin', 'harry potter', 'david copper field','Geranimo', 'moby dick'],
		'cost': [300, 250,550, 300, 600],
		'dateofpublication': [1948, 1990, 'NaN', 1960, 'NaN'],
		'origin': ['usa','uk','uk','usa','usa']}

# Convert into DataFrame
df = pd.DataFrame(data)

# Display data

c = avg = 0
for ele in df['dateofpublication']:
	if str(ele).isnumeric():
		c += 1
		avg += ele
avg /= c
# Replace missing values
df = df.replace(to_replace="NaN",value=avg)
# Display data
df
# Categorize origin
df['origin'] = df['origin'].map({'usa': 2,'uk': 1, }).astype(float)
# Display data
df

# Filter costs greater than 300
df = df[df['cost'] >= 300].copy()
# Remove origin column from filtered DataFrame
df.drop('origin', axis=1, inplace=True)
# Display data
df



# Remove age column from filtered DataFrame
df.drop('origin', axis=1, inplace=True)
# Display data
df

# Filter costs greater than 300
df = df[df['cost'] >= 300].copy()
df

df = pd.DataFrame(data)

df = pd.DataFrame(data)

# Display data

c = avg = 0
for ele in df['dateofpublication']:
	if str(ele).isnumeric():
		c += 1
		avg += ele
avg /= c
# Replace missing values
df = df.replace(to_replace="NaN",value=avg)
df

# Categorize origin
df['origin'] = df['origin'].map({'usa': 12,'uk': 1, }).astype(float)
# Display data
df

mydata=pd.read_csv("DailyDelhiClimateTest.csv")
print(mydata)
#read data
#display data

mydata.head()
#head top 5 lines

mydata.tail()
#last 5 lines

mydata.iloc[0:,:1]
#specific lines or columns

x=mydata.dtypes
print(x)
#gives data types

mydata.info
#gives column data

mydata.shape
#number of columns and rows

print(pd.Series())
#prints in one line



# Convert into DataFrame
df = pd.DataFrame(data)

mydata.mode(axis = 0)
#gives mode

print(mydata.describe())
#gives description of various columns and rows

mydata.mean(axis = 0)
#gives mean of data

mydata.median(axis = 0)
#gives median of data

data34 = {'date': [' 2017-01-01', '2017-01-02', '2017-01-02'],
           'temperature': [' 15.913043', '18.500000', '17.111111']
		}
data22 = {'date': [' 2017-04-20', '2017-05-21', '2017-06-21'],
    'temperature': [' 16', '17', '18']
		}

# Convert into DataFrame
df1 = pd.DataFrame(data34)
df2=pd.DataFrame(data22)
newdf = df1.merge(df2, how='left')
print(newdf)
print(df1.groupby(["temperature"]).mean())
#The merge() method updates the content of two DataFrame by merging them together

data34 = {'date': [' 2017-01-01', '2017-01-02', '2017-01-02'],
           'temperature': [' 15.913043', '18.500000', '17.111111']
		}
data22 = {'date': [' 2017-04-20', '2017-05-21', '2017-06-21'],
    'temperature': [' 16', '17', '18']
		}
df1 = pd.DataFrame(data34)
print(df1.groupby(["temperature"]).mean())

grouped_by = mydata.groupby('meantemp')['humidity'].mean()
print(grouped_by)
#grouped

#SUMMARY
#We have made use of data set from https://www.kaggle.com/datasets/sumanthvrao/daily-climate-time-series-data
#we have also made use of custom data like types of books and also learnt how to replace "Nan " values also learnt how to calculate mean median mode of various data also
#datas in specific columns
#also learnt how to group data and also find mean/mode/median of those datas
#in this assingment we have done data preprocessing,formatting and also normalization,overall a great introductory assingment to learn about data loading,storage and various file formats